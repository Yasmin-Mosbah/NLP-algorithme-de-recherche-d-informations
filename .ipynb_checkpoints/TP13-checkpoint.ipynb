{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in /home/deptinfo/.local/lib/python3.8/site-packages (1.19.5)\n",
      "Requirement already up-to-date: nltk in /home/deptinfo/.local/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/deptinfo/.local/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/deptinfo/.local/lib/python3.8/site-packages (from nltk) (4.50.1)\n",
      "Requirement already satisfied, skipping upgrade: regex in /home/deptinfo/.local/lib/python3.8/site-packages (from nltk) (2020.9.27)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: pandas in /home/deptinfo/.local/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/deptinfo/.local/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/deptinfo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Installation et Import des packages necessaires (pandas, Nltk)\n",
    "!pip3 install --user -U numpy\n",
    "!pip3 install --user -U nltk\n",
    "!pip3 install pandas\n",
    "\n",
    "## Import de packages necessaires pour la suite\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as et \n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import difflib\n",
    "import glob\n",
    "import string \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "lem = WordNetLemmatizer() \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Def des fonctions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitements des docuements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fct qui va parcourir tout les .txt d'un repertoir pour copier leur contenu dans une liste\n",
    "#la fct prends en argument le chemin/nom du repertoir cible\n",
    "#Retourne une liste ou chaque indice est un document\n",
    "def txt_doc_to_string_list(doc):\n",
    "    doc_list = glob.glob(doc + \"/*.txt\")\n",
    "    txt_list = []\n",
    "    doc_list = sorted(doc_list)\n",
    "    for i in range(len(doc_list)):\n",
    "        with open(doc_list[i], 'r') as file:\n",
    "            txt_list.append(file.read().replace('\\n', ' '))\n",
    "    return txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise tout les documents avec nltk\n",
    "def string_to_token(txt_list):\n",
    "    txt_list_token = []\n",
    "    for i in range(len(txt_list)):\n",
    "        doc_token = nltk.word_tokenize((txt_list[i].lower()))\n",
    "        txt_list_token.append(doc_token)\n",
    "    return txt_list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FCt qui supprime la ponctuation, les stop word et les adresses https\n",
    "def remove_punctuation_stopwords(txt_list_token):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    listword = [\"_\",\"-\",\"(\",\")\",\"—\",\"‘\",'’','”',\"https\",\"http\"]\n",
    "    for doc in txt_list_token:\n",
    "        for word in doc[::-1]: \n",
    "            if word in string.punctuation or word in listword or \"//\" in word :\n",
    "                doc.remove(word)\n",
    "            if word in stop_words:\n",
    "                doc.remove(word)\n",
    "    return txt_list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatise tout les documents contenue dans la liste en fonction du leurs tag\n",
    "def lemma_text(docs):\n",
    "    lem_docs = []\n",
    "    for doc in docs:\n",
    "        lem_doc = []\n",
    "        doc_tag = nltk.pos_tag(doc)\n",
    "        for word in doc_tag:\n",
    "            pos_label = (word[1][0]).lower()\n",
    "            if pos_label == 'j': pos_label = 'a'    \n",
    "            if pos_label in ['a', 's', 'v']:\n",
    "                lem_word = lem.lemmatize(word[0], pos=pos_label)\n",
    "            else:   \n",
    "                lem_word = lem.lemmatize(word[0])\n",
    "            lem_doc.append(lem_word)\n",
    "        lem_docs.append(lem_doc)\n",
    "    return lem_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fct qui génère le dictionnaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génère un dictionnaire a partire des documents lemmatisé \n",
    "#Pour cela, je récupérer chaque mots, j'indique dans quel document il apparait et sa fréquence par doc\n",
    "def to_dic(lemma):\n",
    "    dic = pd.DataFrame(columns=['word','document','frequence'])\n",
    "    for i in range(len(lemma)):\n",
    "        list_word = list(dict.fromkeys(lemma[i])) \n",
    "        for word in list_word:\n",
    "            nb_occurance = lemma[i].count(word)\n",
    "            dic = dic.append({'word' : word ,'document' : i ,'frequence' : nb_occurance }, ignore_index=True)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fct qui gènre la matrice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génère la matrice a aprtir du dictionnaitre\n",
    "#la matrice a pour colonne les numero des documents\n",
    "#SI le mot apparait dans le document j'ajoute un \"1\" dans la matrice, sinon un 0\n",
    "def matrice(df) :\n",
    "    #all_word = df['word'].tolist()\n",
    "    all_word = list(dict.fromkeys(df['word'].tolist())) \n",
    "    col = list(dict.fromkeys(df[\"document\"].tolist()))\n",
    "    matrice = pd.DataFrame(columns=col, index = all_word)\n",
    "    for word in all_word:\n",
    "        liste = list(df.loc[df['word']==word][\"document\"])\n",
    "        for val in liste :\n",
    "            matrice.loc[word, val] = 1\n",
    "    #matrice = matrice.fillna(0)\n",
    "    return matrice.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fct qui génère les index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génère les index a aprtir du dictionnaire\n",
    "#Chaque mot sera dans un vecteur ou il y aura simplement le numero du docuement ou il apparait \n",
    "def index_inverse(df):\n",
    "    vect_words = []\n",
    "    all_word = list(dict.fromkeys(df['word'].tolist())) \n",
    "    for i in range(len(all_word)):\n",
    "        vect_words.append([all_word[i],list(df.loc[df['word']==all_word[i]][\"document\"])])\n",
    "    return vect_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requetes Booléennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traite les requetes booleen (tokenisation, lemmatisation, distinction de si on demande un and, un and not ou un or)\n",
    "def traitement_requete_bool(requete):\n",
    "    token_req =  nltk.word_tokenize(requete.lower())\n",
    "    req_tag = nltk.pos_tag(token_req)\n",
    "    lem_req = []\n",
    "    for word in req_tag:\n",
    "        pos_label = (word[1][0]).lower()\n",
    "        if pos_label == 'j': pos_label = 'a'\n",
    "        if pos_label in ['a', 's', 'v']:\n",
    "            lem_word = lem.lemmatize(word[0], pos=pos_label)\n",
    "        else:   # For nouns and everything else as it is the default kwarg\n",
    "            lem_word = lem.lemmatize(word[0])\n",
    "        lem_req.append(lem_word)\n",
    "    req  = []\n",
    "    res = []\n",
    "    search = []\n",
    "    andnot = []\n",
    "    str_req = ' '.join(lem_req)\n",
    "    str_req = str_req.replace(\"( \",\"\")\n",
    "    str_req = str_req.replace(\" )\",\"\")\n",
    "    if \" and not \" in str_req:\n",
    "        tab = str_req.split(\" and not \")\n",
    "        search = tab [0]\n",
    "        andnot = tab [1]\n",
    "    else :\n",
    "        search = str_req\n",
    "    search = search.split(\" and \")\n",
    "\n",
    "    if len(andnot) > 0 :\n",
    "        andnot = andnot.split(\" and \")\n",
    "        for i in range(len(andnot)):\n",
    "            if \" or \" in andnot[i]:\n",
    "                andnot[i] = andnot[i].split(\" or \")\n",
    "            if \" \" in andnot[i]:\n",
    "                tab = andnot[i].split(\" \")\n",
    "                for word in tab:\n",
    "                    andnot.append(word)\n",
    "                andnot.pop(i)\n",
    "    for i in range(len(search)):\n",
    "        if \" or \" in search[i] :\n",
    "            search[i] = search[i].split(\" or \")\n",
    "        if \" \" in search[i]:\n",
    "                tab = search[i].split(\" \")\n",
    "                for word in tab:\n",
    "                    search.append(word)\n",
    "                search.pop(i)\n",
    "    return andnot,search        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applique une requete simple qui récupérer les documents contenant les mots désirés et les classe avec la fréquence des mots\n",
    "def res_requete_bool(req_bool,df):\n",
    "    #Trainement de la requete booléenne\n",
    "    tab_req = traitement_requete_bool(req_bool)\n",
    "    andnot = tab_req[0]\n",
    "    andword = tab_req[1]\n",
    "\n",
    "    wordnot = []\n",
    "    valnot =[]\n",
    "    for pos in andnot:\n",
    "        if isinstance(pos, list):\n",
    "            tab = df[df[\"word\"].isin(pos)]\n",
    "            valnot = tab.document.unique().tolist()\n",
    "      \n",
    "        else :\n",
    "            wordnot.append(pos)\n",
    "    tabnot = df[df[\"word\"].isin(wordnot)]\n",
    "    tabnot2 = tabnot[tabnot.groupby(\"document\")[\"document\"].transform('size') == len(wordnot)]\n",
    "    valnot= valnot + tabnot2.document.unique().tolist()\n",
    "\n",
    "\n",
    "    word = []\n",
    "    val = []\n",
    "    sum_val = []\n",
    "    for pos in andword:\n",
    "        if isinstance(pos, list):\n",
    "            tab2 = df[df[\"word\"].isin(pos)]\n",
    "            #val = tab2.document.unique().tolist()\n",
    "            docs = tab2.groupby(\"document\")['frequence'].sum().index.values.tolist()\n",
    "            freq = tab2.groupby(\"document\")['frequence'].sum().values.tolist()\n",
    "            for i in range(len(docs)):\n",
    "                sum_val.append([docs[i],freq[i]])\n",
    "\n",
    "        else :\n",
    "            word.append(pos)\n",
    "    tabword = df[df[\"word\"].isin(andword)]\n",
    "    #tabword2 = tabword[tabword.groupby(\"document\")[\"document\"].transform('size') == len(word)]\n",
    "    docs = tabword.groupby(\"document\")['frequence'].sum().index.values.tolist()\n",
    "    freq = tabword.groupby(\"document\")['frequence'].sum().values.tolist()\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "\n",
    "        sum_val.append([docs[i],freq[i]])\n",
    "    index = []\n",
    "    for i in range(len(sum_val)):\n",
    "        for val in valnot:\n",
    "            if val == sum_val[i][0]:\n",
    "                index.append(i)\n",
    "    for val in index[::-1]:\n",
    "        sum_val.pop(val)\n",
    "\n",
    "    res = pd.DataFrame(sum_val,columns =['document', 'freq'])\n",
    "\n",
    "    res = res.groupby(\"document\")['freq'].sum().to_frame().sort_values(by ='freq', ascending=False )\n",
    "    \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requetes complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traitement des requete complexe (tokenisation, stop word, lemmatisation..)\n",
    "def traitement_req_complexe(req_comp):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    listword = [\"_\",\"-\",\"(\",\")\",\"—\",\"‘\",'’','”',\"https\",\"http\"]\n",
    "\n",
    "\n",
    "    req =  nltk.word_tokenize(req_comp.lower())\n",
    "    for word in req[::-1]: \n",
    "        if word in string.punctuation or word in listword or \"//\" in word :\n",
    "            req.remove(word)\n",
    "        if word in stop_words:\n",
    "            req.remove(word)\n",
    "\n",
    "    req_tag = nltk.pos_tag(req)\n",
    "    lem_req = []\n",
    "\n",
    "    for word in req_tag:\n",
    "        pos_label = (word[1][0]).lower()\n",
    "        if pos_label == 'j': pos_label = 'a'\n",
    "        if pos_label in ['a', 's', 'v']:\n",
    "            lem_word = lem.lemmatize(word[0], pos=pos_label)\n",
    "        else:   # For nouns and everything else as it is the default kwarg\n",
    "            lem_word = lem.lemmatize(word[0])\n",
    "        lem_req.append(lem_word)\n",
    "    return lem_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcule du TF*IDF\n",
    "#Cette fct ne prends qu'un terme et non une requete entière\n",
    "#Retourne le TFIDF du termes en fonction des documents\n",
    "\n",
    "def tfidf(df, matrice,docs,req):\n",
    "    nb_doc = len(matrice.columns)\n",
    "    nb_doc_terme = matrice.sum(axis=1)\n",
    "    tabword = df[df[\"word\"] == req ]\n",
    "    wordfreq = 0\n",
    "    for word, freq in nb_doc_terme.items():\n",
    "        if word == req:\n",
    "            wordfreq = freq\n",
    "    tabword = tabword.drop(columns=['word'])        \n",
    "    records = tabword.to_records(index=False)\n",
    "    result = list(records)\n",
    "\n",
    "    res_cal = []\n",
    "    for i in range(len(result)):\n",
    "        res = (result[i][1]/len(docs[i]))*(math.log((nb_doc/wordfreq),2)+1)\n",
    "        res_cal.append([result[i][0],res])\n",
    "    return res_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classement_doc_req_comp(df,matrice,docs,req_tr):\n",
    "    #Trainement de la requete complexe\n",
    "    req= traitement_req_complexe(req_tr)\n",
    "    columns = []\n",
    "    df_ = pd.DataFrame(index=matrice.columns, columns=columns)\n",
    "    for i in range(len(req)):\n",
    "        tab = tfidf(df, matrice,docs,req[i])\n",
    "        for j in range(len(tab)):\n",
    "            df_.loc[tab[j][0], req[i]] = tab[j][1]\n",
    "    #Je supprime les lignes contenant des NaN car cela veut dire que ces documents ne contiennent pas tout les mots requis\n",
    "    #df_ = df_.dropna()\n",
    "    classement = df_.sum(axis=1).to_frame(\"freq\").sort_values(by ='freq', ascending=False )\n",
    "    classement = classement[classement.freq > 0.0]\n",
    "    return classement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application des fct réalisées plus haut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement du document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FCt qui regroupe toutes les fct précédente qui traitent le document donc il y a la tokenisation, la suppresion de la ponctuation, stop word, lemmatisation...\n",
    "def traitement_doc(nomdoc):\n",
    "    #Extraction des txt dans le repertoire documents pour les convertirs en une liste de String (la list contient tout les documents)\n",
    "    txtlist = txt_doc_to_string_list(\"documents\")\n",
    "    #Transformation en token\n",
    "    txtlist_token = string_to_token(txtlist)\n",
    "    #Suppréssion des stopword et de la punctuation\n",
    "    txtlist_token_nostop = remove_punctuation_stopwords(txtlist_token)\n",
    "    #Lemmatisation\n",
    "    lemma_txt = lemma_text(txtlist_token_nostop)\n",
    "    \n",
    "    return lemma_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération de la liste des documents avec des mots lemmatisé et génération du dictionnaire, de la matrice et des index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traitement des documents en retournant une liste contenant la liste des mots lemmatisé par document\n",
    "lemma = traitement_doc(\"documents\")\n",
    "#Dictionnaire relatif a la liste des documents\n",
    "dic_txt = to_dic(lemma)\n",
    "#Matrice d'incidence a partir du dictionnaire\n",
    "matrice_incidence = matrice(dic_txt)\n",
    "#Index inverse a partir du dictionnaire\n",
    "ind_inverse = index_inverse(dic_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application de la fonction qui classe les documents en fonction du de la frequence des mots  (requete booléenne)\n",
    "### J'ai déjà écrit les requetes de l'énoncé, il vous suffira de changer le numero de la requete dans les arguements de la fonction \"res_requete_bool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['genome', 'variant']], ['antibody', 'old', 'adult'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Requete booléenne de l'énoncé :\n",
    "requete_bool1 = \"desease AND severe AND pneumonia\"\n",
    "requete_bool2 = \"antibody AND plasma AND (cells OR receptors)\"\n",
    "requete_bool3 = \"antimalarial drugs OR antiviral agents OR immunomodulators\"\n",
    "requete_bool4 = \"NOT plasma AND risk of infection AND restrictions\"\n",
    "requete_bool5 = \"(older adults AND antibodies) AND NOT (genomes OR variant)\"\n",
    "\n",
    "#df contenant tout les documents répondant a la requete, du meilleur au plus mauvais\n",
    "doc_freq_bool = res_requete_bool(requete_bool2,dic_txt) ## Changez le premier argument de cette fonction en fonction avec la requete que vous voulez tester\n",
    "doc_freq_bool\n",
    "\n",
    "traitement_requete_bool(requete_bool5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Application de la fonction qui classe les documents en fonction tf*idf  (requete complexe)\n",
    "### J'ai déjà écrit les requetes de l'énoncé, il vous suffira de changer le numero de la requete dans les arguements de la fonction \"res_requete_bool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.085381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.040799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       freq\n",
       "8  0.085381\n",
       "7  0.040799\n",
       "1  0.007646\n",
       "2  0.004776\n",
       "0  0.003754\n",
       "6  0.001793\n",
       "9  0.001539\n",
       "3  0.001425\n",
       "4  0.000494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Requete complexe de l'énoncé :\n",
    "req_comp1 = \"antibody treatments\"\n",
    "req_comp2 = \"efficacy and safety of the treatments\"\n",
    "req_comp3 = \"family access to hospitals\"\n",
    "req_comp4 = \"contact tracing results\"\n",
    "req_comp5 = \"genomic analysis of SARS-CoV-2 disease\"\n",
    "\n",
    "#df contenant tout les documents répondant a la requete, du meilleur au plus mauvais\n",
    "doc_freq_comp = classement_doc_req_comp(dic_txt,matrice_incidence,lemma,req_comp3)  ## Changez le dernier argument de cette fonction en fonction avec la requete que vous voulez tester\n",
    "doc_freq_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour finir vous pouvez visualiser le dictionnaire, la matrice et les index :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionnaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>document</th>\n",
       "      <th>frequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>january</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mutation</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undercut</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>immune</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>beta-coronaviruses</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9513</th>\n",
       "      <td>reveal</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9514</th>\n",
       "      <td>apply</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9515</th>\n",
       "      <td>variety</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9516</th>\n",
       "      <td>rsv</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9517 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word document frequence\n",
       "0                     13        0        12\n",
       "1                january        0        14\n",
       "2               mutation        0        43\n",
       "3               undercut        0         1\n",
       "4                 immune        0       103\n",
       "...                  ...      ...       ...\n",
       "9512  beta-coronaviruses        9         1\n",
       "9513              reveal        9         1\n",
       "9514               apply        9         1\n",
       "9515             variety        9         1\n",
       "9516                 rsv        9         1\n",
       "\n",
       "[9517 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice d'incidence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>january</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mutation</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undercut</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immune</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interplay</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cross-immunity</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha-</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta-coronaviruses</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsv</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5556 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0  1  2  3  4  5  6  7  8  9\n",
       "13                  1  1  1  1  0  0  0  0  1  0\n",
       "january             1  1  0  0  0  0  0  0  0  0\n",
       "mutation            1  0  0  0  0  0  0  0  0  0\n",
       "undercut            1  0  0  0  0  0  0  0  0  0\n",
       "immune              1  0  1  1  0  0  1  0  0  1\n",
       "...                .. .. .. .. .. .. .. .. .. ..\n",
       "interplay           0  0  0  0  0  0  0  0  0  1\n",
       "cross-immunity      0  0  0  0  0  0  0  0  0  1\n",
       "alpha-              0  0  0  0  0  0  0  0  0  1\n",
       "beta-coronaviruses  0  0  0  0  0  0  0  0  0  1\n",
       "rsv                 0  0  0  0  0  0  0  0  0  1\n",
       "\n",
       "[5556 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrice_incidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list des Index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['13', [0, 1, 2, 3, 8]],\n",
       " ['january', [0, 1]],\n",
       " ['mutation', [0]],\n",
       " ['undercut', [0]],\n",
       " ['immune', [0, 2, 3, 6, 9]],\n",
       " ['response', [0, 2, 3, 4, 7, 8, 9]],\n",
       " ['covid', [0, 7, 9]],\n",
       " ['virus', [0, 1, 2, 3, 4, 5, 9]],\n",
       " ['handful', [0]],\n",
       " ['sars-cov-2', [0, 1, 2, 3, 4, 5, 6, 8, 9]],\n",
       " ['help', [0, 1, 2, 4]],\n",
       " ['escape', [0, 9]],\n",
       " ['mount', [0]],\n",
       " ['subset', [0]],\n",
       " ['infect', [0, 1, 3, 9]],\n",
       " ['people', [0, 5, 7, 9]],\n",
       " ['researcher', [0, 4, 8]],\n",
       " ['identify', [0, 1, 2, 3, 8]],\n",
       " ['thousand', [0]],\n",
       " ['sample', [0, 1, 3, 4, 8]],\n",
       " ['vast', [0]],\n",
       " ['majority', [0, 4, 6]],\n",
       " ['unlikely', [0, 3]],\n",
       " ['much', [0, 7, 9]],\n",
       " ['effect', [0, 1, 2, 3, 4, 7, 8, 9]],\n",
       " ['biology', [0]],\n",
       " ['potentially', [0, 1, 7, 8, 9]],\n",
       " ['important', [0, 1, 3, 4, 8, 9]],\n",
       " ['jesse', [0]],\n",
       " ['bloom', [0]],\n",
       " ['fred', [0]],\n",
       " ['hutchinson', [0]],\n",
       " ['cancer', [0, 2]],\n",
       " ['research', [0, 2, 3, 4, 8]],\n",
       " ['center', [0, 1, 2, 3, 4, 5]],\n",
       " ['seattle', [0]],\n",
       " ['washington', [0]],\n",
       " ['colleague', [0, 8]],\n",
       " ['study', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['antibody', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['isolated', [0]],\n",
       " ['blood', [0, 1, 2, 8]],\n",
       " ['serum', [0, 1, 3, 6, 8]],\n",
       " ['recover', [0, 1, 3, 4, 8]],\n",
       " ['covid-19', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['a.', [0, 1, 5]],\n",
       " ['j.', [0]],\n",
       " ['greaney', [0]],\n",
       " ['et', [0]],\n",
       " ['al', [0]],\n",
       " ['preprint', [0]],\n",
       " ['biorxiv', [0]],\n",
       " ['2021', [0, 1, 8]],\n",
       " ['team', [0, 1, 3, 8]],\n",
       " ['test', [0, 1, 2, 3, 4, 5, 6, 8]],\n",
       " ['spike', [0, 1, 3, 4, 8]],\n",
       " ['protein', [0, 1, 3, 4, 8]],\n",
       " ['carry', [0, 6]],\n",
       " ['different', [0, 1, 2, 6, 7, 8, 9]],\n",
       " ['version', [0, 2, 3, 4, 8]],\n",
       " ['region', [0, 1, 2, 6, 7]],\n",
       " ['call', [0]],\n",
       " ['receptor', [0, 2, 4, 8, 9]],\n",
       " ['bind', [0, 4, 8, 9]],\n",
       " ['domain', [0, 3, 4, 8]],\n",
       " ['rbd', [0, 8]],\n",
       " ['recognize', [0, 9]],\n",
       " ['host', [0, 6, 9]],\n",
       " ['cell', [0, 1, 3, 4, 8, 9]],\n",
       " ['major', [0, 3, 4, 8, 9]],\n",
       " ['target', [0, 1, 3, 4, 8, 9]],\n",
       " ['reduced', [0]],\n",
       " ['ability', [0]],\n",
       " ['tightly', [0]],\n",
       " ['change', [0, 3, 4, 5, 8, 9]],\n",
       " ['might', [0, 6, 7, 8, 9]],\n",
       " ['also', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['indicate', [0, 2, 3, 4, 5, 8, 9]],\n",
       " ['reduction', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['disable', [0]],\n",
       " ['vary', [0, 2, 4, 9]],\n",
       " ['substantially', [0, 3, 4, 7]],\n",
       " ['consequential', [0]],\n",
       " ['location', [0]],\n",
       " ['e484', [0]],\n",
       " ['cause', [0, 1, 2, 4, 5, 6, 8, 9]],\n",
       " ['steep', [0]],\n",
       " ['drop', [0, 9]],\n",
       " ['potency', [0]],\n",
       " ['individual', [0, 1, 3, 5, 6, 7, 8, 9]],\n",
       " ['coronavirus', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['variant', [0, 4]],\n",
       " ['south', [0, 2, 6]],\n",
       " ['africa', [0, 2]],\n",
       " ['brazil', [0, 2]],\n",
       " ['spot', [0, 5]],\n",
       " ['finding', [0, 1, 3, 4, 6, 8, 9]],\n",
       " ['yet', [0, 2, 3, 4, 8]],\n",
       " ['peer', [0]],\n",
       " ['review', [0, 1, 2, 3, 4, 5]],\n",
       " ['12', [0, 1, 4, 8, 9]],\n",
       " ['go', [0]],\n",
       " ['wild', [0]],\n",
       " ['tie', [0]],\n",
       " ['lung', [0, 4, 6, 8]],\n",
       " ['damage', [0, 6, 8]],\n",
       " ['severe', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['respiratory', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['symptom', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['seem', [0, 7]],\n",
       " ['result', [0, 1, 2, 3, 4, 5, 6, 8, 9]],\n",
       " ['activity', [0, 3, 6, 8]],\n",
       " ['specific', [0, 1, 2, 4, 9]],\n",
       " ['long-term', [0, 1, 3, 6, 8, 9]],\n",
       " ['inflammation', [0]],\n",
       " ['alexander', [0]],\n",
       " ['misharin', [0]],\n",
       " ['northwestern', [0, 4]],\n",
       " ['university', [0, 4, 5]],\n",
       " ['evanston', [0]],\n",
       " ['illinois', [0]],\n",
       " ['examine', [0, 1, 3, 4]],\n",
       " ['fluid', [0]],\n",
       " ['88', [0, 1]],\n",
       " ['pneumonia', [0, 2, 6, 8]],\n",
       " ['infection', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['r.', [0, 1]],\n",
       " ['grant', [0, 1]],\n",
       " ['nature', [0, 9]],\n",
       " ['high', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['number', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['certain', [0, 3]],\n",
       " ['type', [0, 1, 2, 3, 9]],\n",
       " ['class', [0, 7]],\n",
       " ['lungs', [0]],\n",
       " ['find', [0, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['nearly', [0, 4, 8]],\n",
       " ['70', [0, 4]],\n",
       " ['alveolar', [0]],\n",
       " ['macrophage', [0]],\n",
       " ['locate', [0]],\n",
       " ['tiny', [0]],\n",
       " ['air', [0, 1, 2, 4]],\n",
       " ['sacs', [0]],\n",
       " ['contain', [0, 3, 9]],\n",
       " ['harbour', [0]],\n",
       " ['show', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['relatively', [0, 9]],\n",
       " ['expression', [0]],\n",
       " ['gene', [0, 4]],\n",
       " ['involve', [0, 1, 2, 3, 4]],\n",
       " ['suggest', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['reach', [0, 1, 2, 4, 9]],\n",
       " ['respond', [0]],\n",
       " ['produce', [0, 9]],\n",
       " ['inflammatory', [0, 2, 8]],\n",
       " ['molecule', [0]],\n",
       " ['attract', [0]],\n",
       " ['turn', [0, 7]],\n",
       " ['stimulates', [0]],\n",
       " ['make', [0, 2, 3, 8, 9]],\n",
       " ['persistent', [0, 1, 8]],\n",
       " ['could', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['lead', [0, 2, 3, 4, 7, 9]],\n",
       " ['life-threatening', [0, 1, 4]],\n",
       " ['consequence', [0, 8, 9]],\n",
       " ['11', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['traitorous', [0]],\n",
       " ['link', [0, 4]],\n",
       " ['death', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['normally', [0]],\n",
       " ['attack', [0, 9]],\n",
       " ['pathogen', [0, 7, 9]],\n",
       " ['sometimes', [0, 7]],\n",
       " ['rogue', [0]],\n",
       " ['instead', [0, 5, 7]],\n",
       " ['besiege', [0]],\n",
       " ['bodily', [0]],\n",
       " ['component', [0, 3, 9]],\n",
       " ['new', [0, 1, 2, 3, 4, 5, 7, 8, 9]],\n",
       " ['add', [0, 2, 7, 8]],\n",
       " ['grow', [0]],\n",
       " ['body', [0, 2]],\n",
       " ['autoantibody', [0]],\n",
       " ['poor', [0, 7, 8]],\n",
       " ['outcome', [0, 1, 2, 3, 4, 5, 8, 9]],\n",
       " ['ana', [0, 1]],\n",
       " ['rodriguez', [0]],\n",
       " ['david', [0]],\n",
       " ['lee', [0]],\n",
       " ['nyu', [0]],\n",
       " ['grossman', [0]],\n",
       " ['school', [0, 4, 5, 8]],\n",
       " ['medicine', [0, 4, 5]],\n",
       " ['york', [0]],\n",
       " ['city', [0]],\n",
       " ['level', [0, 1, 2, 4, 5, 7, 8, 9]],\n",
       " ['collect', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['86', [0, 8]],\n",
       " ['require', [0, 4, 6, 8, 9]],\n",
       " ['hospitalization', [0, 1, 2, 3, 4]],\n",
       " ['particularly', [0, 1, 3, 6, 8]],\n",
       " ['interested', [0]],\n",
       " ['annexin', [0]],\n",
       " ['a2', [0]],\n",
       " ['stabilize', [0]],\n",
       " ['cell-membrane', [0]],\n",
       " ['structure', [0]],\n",
       " ['play', [0, 2]],\n",
       " ['part', [0, 3, 8]],\n",
       " ['ensure', [0, 2, 4, 8]],\n",
       " ['integrity', [0, 8]],\n",
       " ['vessel', [0]],\n",
       " ['block', [0, 1, 4, 9]],\n",
       " ['injury', [0, 4, 6, 8]],\n",
       " ['hallmark', [0]],\n",
       " ['scientist', [0]],\n",
       " ['anti-annexin', [0]],\n",
       " ['average', [0, 3]],\n",
       " ['eventually', [0, 9]],\n",
       " ['die', [0, 1, 2, 6, 7, 8]],\n",
       " ['survive', [0, 2]],\n",
       " ['difference', [0, 1, 2, 3, 4, 7, 8]],\n",
       " ['statistically', [0, 6, 8]],\n",
       " ['significant', [0, 1, 2, 8, 9]],\n",
       " ['m.', [0, 1]],\n",
       " ['zuniga', [0]],\n",
       " ['medrxiv', [0]],\n",
       " ['necessary', [0, 8, 9]],\n",
       " ['establish', [0]],\n",
       " ['clear', [0, 1, 3, 4, 9]],\n",
       " ['causal', [0]],\n",
       " ['rare', [0]],\n",
       " ['8', [0, 1, 2, 6, 8, 9]],\n",
       " ['quick', [0]],\n",
       " ['treatment', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['antibody-laden', [0]],\n",
       " ['cut', [0]],\n",
       " ['risk', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['clinical', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['trial', [0, 1, 2, 3, 4, 8]],\n",
       " ['old', [0, 1, 2, 3, 4, 9]],\n",
       " ['adult', [0, 1, 5, 8, 9]],\n",
       " ['early', [0, 1, 3, 4, 7, 8, 9]],\n",
       " ['dose', [0, 2, 3, 4]],\n",
       " ['plasma', [0, 1, 3, 4, 8]],\n",
       " ['prevent', [0, 1, 3, 4, 5, 9]],\n",
       " ['progression', [0, 1, 2, 4]],\n",
       " ['disease', [0, 1, 2, 3, 4, 5, 6, 8, 9]],\n",
       " ['contains', [0, 4]],\n",
       " ['antibodies', [0, 1, 8]],\n",
       " ['mixed', [0, 2, 3, 4]],\n",
       " ['need', [0, 1, 2, 3, 5, 8, 9]],\n",
       " ['give', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['course', [0, 1, 2, 3, 4, 6]],\n",
       " ['effective', [0, 1, 3, 5, 6, 9]],\n",
       " ['fernando', [0]],\n",
       " ['polack', [0]],\n",
       " ['fundación', [0, 1]],\n",
       " ['infant', [0, 1]],\n",
       " ['buenos', [0, 1]],\n",
       " ['conduct', [0, 1, 2, 3, 4]],\n",
       " ['rigorous', [0]],\n",
       " ['ass', [0, 1, 7, 8]],\n",
       " ['within', [0, 1, 2, 3, 4, 7, 8, 9]],\n",
       " ['72', [0, 1, 3, 6]],\n",
       " ['hour', [0, 1, 2, 3, 4, 7]],\n",
       " ['onset', [0, 1, 3, 4, 8]],\n",
       " ['participant', [0, 8]],\n",
       " ['include', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['age', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['75', [0, 1, 2, 8]],\n",
       " ['65', [0, 1, 2, 4]],\n",
       " ['74', [0, 1]],\n",
       " ['least', [0, 1, 2, 3, 4, 5, 6, 8]],\n",
       " ['one', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['pre-existing', [0, 6]],\n",
       " ['condition', [0, 1, 2, 3, 4, 5, 8]],\n",
       " ['diabetes', [0, 1, 8]],\n",
       " ['libster', [0, 1]],\n",
       " ['n.', [0]],\n",
       " ['engl', [0]],\n",
       " ['med', [0]],\n",
       " ['develop', [0, 1, 3, 4, 5, 8, 9]],\n",
       " ['16', [0, 1, 2, 3, 8]],\n",
       " ['80', [0, 1, 2, 3, 4, 8]],\n",
       " ['receive', [0, 1, 2, 3, 4, 5, 7, 8]],\n",
       " ['31', [0, 1, 6, 8]],\n",
       " ['placebo', [0, 1, 2, 3, 4]],\n",
       " ['group', [0, 1, 2, 3, 4, 5, 7, 8, 9]],\n",
       " ['donor', [0, 1]],\n",
       " ['concentration', [0, 1, 3, 8]],\n",
       " ['associate', [0, 1, 2, 3, 7, 8, 9]],\n",
       " ['great', [0, 1, 2, 3, 5, 7, 8]],\n",
       " ['provide', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['evidence', [0, 4, 8, 9]],\n",
       " ['responsible', [0, 3, 6]],\n",
       " ['therapeutic', [0, 1, 3, 6]],\n",
       " ['7', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['swift', [0]],\n",
       " ['spread', [0, 4, 5, 6, 9]],\n",
       " ['two', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['independent', [0, 1, 3]],\n",
       " ['analysis', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['overtake', [0]],\n",
       " ['united', [0, 2, 4]],\n",
       " ['kingdom', [0]],\n",
       " ['indeed', [0, 9]],\n",
       " ['transmissible', [0, 9]],\n",
       " ['form', [0, 1, 3, 4, 8, 9]],\n",
       " ['eric', [0]],\n",
       " ['volz', [0]],\n",
       " ['neil', [0]],\n",
       " ['ferguson', [0]],\n",
       " ['imperial', [0, 4]],\n",
       " ['college', [0, 8]],\n",
       " ['london', [0]],\n",
       " ['2,000', [0]],\n",
       " ['genome', [0]],\n",
       " ['label', [0]],\n",
       " ['concern', [0, 2, 8]],\n",
       " ['202012/01', [0]],\n",
       " ['october', [0, 1, 4]],\n",
       " ['december', [0, 2, 3]],\n",
       " ['2020.', [0, 1, 2, 3, 8]],\n",
       " ['analyse', [0, 2, 3, 8]],\n",
       " ['roughly', [0]],\n",
       " ['275,000', [0]],\n",
       " ['uk', [0]],\n",
       " ['administer', [0, 1, 2, 3, 4, 6]],\n",
       " ['late', [0, 1, 4]],\n",
       " ['2020', [0, 1, 2, 3, 4, 5, 6, 8]],\n",
       " ['e.', [0]],\n",
       " ['estimate', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['frequency', [0, 4, 7, 9]],\n",
       " ['time', [0, 1, 2, 3, 4, 5, 7, 8, 9]],\n",
       " ['author', [0, 1, 2, 3, 4, 8]],\n",
       " ['conclude', [0]],\n",
       " ['50', [0, 1, 2, 5]],\n",
       " ['lockdown', [0, 6]],\n",
       " ['november', [0, 1, 2]],\n",
       " ['curb', [0, 9]],\n",
       " ['case', [0, 1, 2, 5, 6, 7, 8, 9]],\n",
       " ['viral', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['rise', [0, 9]],\n",
       " ['separate', [0, 3, 7, 9]],\n",
       " ['use', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['genomic', [0]],\n",
       " ['data', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['last', [0, 1, 2, 4, 9]],\n",
       " ['month', [0, 1, 3, 7, 8, 9]],\n",
       " ['nicholas', [0]],\n",
       " ['davy', [0]],\n",
       " ['hygiene', [0]],\n",
       " ['tropical', [0]],\n",
       " ['56', [0, 3, 4, 8]],\n",
       " ['6', [0, 1, 2, 3, 4, 7, 8, 9]],\n",
       " ['less-sensitive', [0]],\n",
       " ['outbreaks', [0]],\n",
       " ['rapid', [0, 1, 3, 6, 9]],\n",
       " ['trade', [0]],\n",
       " ['away', [0]],\n",
       " ['degree', [0, 2]],\n",
       " ['reliability', [0]],\n",
       " ['speed', [0, 5]],\n",
       " ['prove', [0]],\n",
       " ['valuable', [0, 6]],\n",
       " ['public-health', [0]],\n",
       " ['tool', [0, 1, 8, 9]],\n",
       " ['community', [0, 5, 7]],\n",
       " ['hit', [0]],\n",
       " ['hard', [0, 7, 9]],\n",
       " ['week', [0, 7]],\n",
       " ['diane', [0]],\n",
       " ['havlir', [0]],\n",
       " ['california', [0, 4]],\n",
       " ['san', [0]],\n",
       " ['francisco', [0]],\n",
       " ['3,300', [0]],\n",
       " ['g.', [0]],\n",
       " ['pilarowski', [0]],\n",
       " ['clin', [0]],\n",
       " ['inf', [0]],\n",
       " ['dis', [0]],\n",
       " ['volunteer', [0]],\n",
       " ['gold-standard', [0]],\n",
       " ['pcr', [0, 4, 6]],\n",
       " ['typically', [0]],\n",
       " ['return', [0, 1, 7]],\n",
       " ['four', [0, 1, 4, 6, 7, 9]],\n",
       " ['day', [0, 1, 2, 3, 4, 7, 8]],\n",
       " ['state', [0, 1, 2, 4, 5, 9]],\n",
       " ['detect', [0, 1, 2]],\n",
       " ['antigen', [0]],\n",
       " ['binaxnow', [0]],\n",
       " ['abbott', [0]],\n",
       " ['laboratory', [0, 1, 2, 4, 8]],\n",
       " ['park', [0]],\n",
       " ['89', [0]],\n",
       " ['237', [0]],\n",
       " ['positive', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['rapid-test', [0]],\n",
       " ['phone', [0]],\n",
       " ['advise', [0]],\n",
       " ['isolate', [0]],\n",
       " ['mean', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['le', [0, 1, 4, 8]],\n",
       " ['likely', [0, 2, 7, 8, 9]],\n",
       " ['wait', [0]],\n",
       " ['approximately', [0, 1, 3, 4, 8, 9]],\n",
       " ['1', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['confirm', [0, 2, 3, 4, 6, 8]],\n",
       " ['meaning', [0]],\n",
       " ['wrong', [0]],\n",
       " ['disclose', [0]],\n",
       " ['receives', [0]],\n",
       " ['non-financial', [0]],\n",
       " ['support', [0, 1, 2, 3, 4, 7, 8]],\n",
       " ['relate', [0, 1, 3, 8, 9]],\n",
       " ['paper', [0, 1]],\n",
       " ['4', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['―', [0]],\n",
       " ['vaccine', [0, 4, 5, 9]],\n",
       " ['work', [0, 1, 5, 8]],\n",
       " ['quickly', [0, 7, 9]],\n",
       " ['ward', [0, 7]],\n",
       " ['rna-based', [0]],\n",
       " ['recently', [0, 1, 3, 4, 7]],\n",
       " ['approve', [0, 1, 2, 8]],\n",
       " ['u', [0, 3, 8, 9]],\n",
       " ['regulator', [0]],\n",
       " ['protection', [0, 1, 2, 4, 5, 9]],\n",
       " ['first', [0, 1, 2, 3, 8, 9]],\n",
       " ['accord', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['large', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['18', [0, 1, 2, 3, 8, 9]],\n",
       " ['food', [0, 1, 4]],\n",
       " ['drug', [0, 1, 3, 4, 6, 8]],\n",
       " ['administration', [0, 1, 3, 4, 5]],\n",
       " ['emergency-use', [0]],\n",
       " ['authorization', [0, 1]],\n",
       " ['moderna', [0, 5]],\n",
       " ['cambridge', [0]],\n",
       " ['massachusetts', [0]],\n",
       " ['shortly', [0]],\n",
       " ['thereafter', [0]],\n",
       " ['lindsey', [0]],\n",
       " ['baden', [0]],\n",
       " ['brigham', [0]],\n",
       " ['woman', [0, 1, 4, 6, 8]],\n",
       " ['hospital', [0, 1, 2, 4, 6, 7, 8, 9]],\n",
       " ['boston', [0, 2]],\n",
       " ['hana', [0]],\n",
       " ['el', [0]],\n",
       " ['sahly', [0]],\n",
       " ['baylor', [0, 4]],\n",
       " ['houston', [0, 1]],\n",
       " ['texas', [0]],\n",
       " ['publish', [0, 1, 3, 4, 8]],\n",
       " ['enrol', [0, 2, 3, 8]],\n",
       " ['30,000', [0]],\n",
       " ['half', [0]],\n",
       " ['dos', [0, 2, 3, 4]],\n",
       " ['28', [0, 1, 2, 4, 6, 8, 9]],\n",
       " ['apart', [0, 8]],\n",
       " ['l.', [0, 1]],\n",
       " ['94', [0, 2, 8]],\n",
       " ['symptomatic', [0, 2, 3, 6]],\n",
       " ['preliminary', [0, 9]],\n",
       " ['hint', [0]],\n",
       " ['defence', [0]],\n",
       " ['asymptomatic', [0, 1, 4, 6, 7, 8, 9]],\n",
       " ['write', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['30', [0, 1, 2, 3, 6, 8]],\n",
       " ['arm', [0]],\n",
       " ['experienced', [0, 8]],\n",
       " ['side', [0, 1]],\n",
       " ['headaches', [0]],\n",
       " ['second', [0, 8]],\n",
       " ['serious', [0, 1, 2, 3, 4, 6]],\n",
       " ['occur', [0, 1, 2, 3, 4, 7, 8, 9]],\n",
       " ['frequently', [0, 4, 6, 9]],\n",
       " ['vaccinate', [0, 5]],\n",
       " ['21', [0, 2, 4, 8, 9]],\n",
       " ['90', [0, 3, 5, 8]],\n",
       " ['french', [0]],\n",
       " ['evade', [0]],\n",
       " ['detection', [0, 1, 3, 9]],\n",
       " ['france', [0]],\n",
       " ['end', [0, 1, 3, 4, 5, 7]],\n",
       " ['nine', [0, 6]],\n",
       " ['resident', [0, 1, 2, 5, 6]],\n",
       " ['undetected', [0]],\n",
       " ['every', [0]],\n",
       " ['person', [0, 1, 2, 3, 4, 9]],\n",
       " ['despite', [0, 2, 3, 7]],\n",
       " ['nationwide', [0, 6]],\n",
       " ['surveillance', [0, 2, 9]],\n",
       " ['programme', [0]],\n",
       " ['reopen', [0]],\n",
       " ['may', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['adopt', [0, 7]],\n",
       " ['strategy', [0, 1, 5, 6, 9]],\n",
       " ['contact', [0, 2, 6, 8, 9]],\n",
       " ['trace', [0]],\n",
       " ['isolation', [0, 5, 7, 8]],\n",
       " ['keep', [0, 6]],\n",
       " ['check', [0]],\n",
       " ['vittoria', [0]],\n",
       " ['colizza', [0]],\n",
       " ['pierre', [0]],\n",
       " ['louis', [0]],\n",
       " ['institute', [0, 2, 3, 4, 6]],\n",
       " ['epidemiology', [0]],\n",
       " ['public', [0, 2, 5, 9]],\n",
       " ['health', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['paris', [0]],\n",
       " ['model', [0, 2, 3, 4, 8, 9]],\n",
       " ['transmission', [0, 7, 9]],\n",
       " ['mid-may', [0]],\n",
       " ['june', [0, 1, 3, 4, 8]],\n",
       " ['national', [0, 2, 4, 5, 8]],\n",
       " ['campaign', [0, 1]],\n",
       " ['miss', [0, 8]],\n",
       " ['90,000', [0]],\n",
       " ['symptoms', [0, 1, 4]],\n",
       " ['country', [0, 1, 2]],\n",
       " ['decline', [0, 4, 8]],\n",
       " ['pullano', [0]],\n",
       " ['low', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['rate', [0, 1, 2, 4, 5, 7, 8, 9]],\n",
       " ['always', [0]],\n",
       " ['equate', [0, 4]],\n",
       " ['many', [0, 1, 4, 7, 9]],\n",
       " ['seek', [0, 3]],\n",
       " ['medical', [0, 1, 3, 4, 5, 8]],\n",
       " ['advice', [0]],\n",
       " ['implement', [0, 1, 7]],\n",
       " ['aggressive', [0]],\n",
       " ['efficient', [0, 8]],\n",
       " ['suspected', [0]],\n",
       " ['useful', [0, 3, 4, 6]],\n",
       " ['fight', [0]],\n",
       " ['pandemic', [0, 1, 2, 4, 5, 6, 7, 8, 9]],\n",
       " ['say', [0, 7]],\n",
       " ['stay-at-home', [0]],\n",
       " ['order', [0, 1, 2, 8]],\n",
       " ['limit', [0, 1, 2, 3, 5, 6, 7, 8]],\n",
       " ['value', [0, 4, 8]],\n",
       " ['41', [0, 3, 4, 8]],\n",
       " ['3', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['measure', [0, 2, 3, 4, 8, 9]],\n",
       " ['closure', [0]],\n",
       " ['restrict', [0, 7]],\n",
       " ['gathering', [0]],\n",
       " ['10', [0, 1, 4, 6, 9]],\n",
       " ['shut', [0]],\n",
       " ['business', [0]],\n",
       " ['action', [0]],\n",
       " ['bring', [0]],\n",
       " ['marginal', [0]],\n",
       " ['benefit', [0, 1, 2, 3, 7, 8]],\n",
       " ['question', [0, 2, 8, 9]],\n",
       " ['linger', [0]],\n",
       " ['relative', [0, 1, 2, 3]],\n",
       " ['effectiveness', [0]],\n",
       " ['reduce', [0, 1, 2, 3, 4, 7, 8, 9]],\n",
       " ['pinpoint', [0]],\n",
       " ['jan', [0, 8]],\n",
       " ['brauner', [0]],\n",
       " ['oxford', [0]],\n",
       " ['22', [0, 1, 8, 9]],\n",
       " ['either', [0, 1, 2, 3, 4]],\n",
       " ['ease', [0]],\n",
       " ['restriction', [0, 6, 7, 8]],\n",
       " ['science', [0, 4, 5, 8]],\n",
       " ['seven', [0, 8]],\n",
       " ['common', [0, 2, 7, 8, 9]],\n",
       " ['anti-transmission', [0]],\n",
       " ['combine', [0, 3]],\n",
       " ['set', [0, 1, 3, 8]],\n",
       " ['closing', [0]],\n",
       " ['“', [0, 1, 3, 5]],\n",
       " ['dampen', [0]],\n",
       " ['close', [0, 5, 6, 9]],\n",
       " ['succession', [0]],\n",
       " ['impossible', [0, 1]],\n",
       " ['disentangle', [0]],\n",
       " ['little', [0, 7, 9]],\n",
       " ['self-sabotaging', [0]],\n",
       " ['usually', [0]],\n",
       " ['occasionally', [0]],\n",
       " ['system', [0, 1, 2, 3, 6, 9]],\n",
       " ['erroneously', [0]],\n",
       " ['organ', [0, 1, 6, 8]],\n",
       " ['even', [0, 3, 4, 7, 8, 9]],\n",
       " ['explain', [0, 2]],\n",
       " ['reaction', [0, 3, 4]],\n",
       " ['akiko', [0]],\n",
       " ['iwasaki', [0]],\n",
       " ['aaron', [0]],\n",
       " ['ring', [0]],\n",
       " ['yale', [0, 5]],\n",
       " ['connecticut', [0]],\n",
       " ['194', [0]],\n",
       " ['seriously', [0]],\n",
       " ['ill', [0, 1, 2, 5, 8]],\n",
       " ['y.', [0]],\n",
       " ['wang', [0, 8]],\n",
       " ['hamper', [0]],\n",
       " ['others', [0, 1, 8]],\n",
       " ['central', [0, 1, 2, 3, 4, 6]],\n",
       " ['nervous', [0]],\n",
       " ['heart', [0, 8]],\n",
       " ['liver', [0, 8]],\n",
       " ['connective', [0, 6]],\n",
       " ['tissue', [0, 6]],\n",
       " ['single', [0, 3, 4, 7]],\n",
       " ['enough', [0, 7, 9]],\n",
       " ['used', [0, 9]],\n",
       " ['distinguish', [0]],\n",
       " ['uninfected', [0]],\n",
       " ['diversity', [0]],\n",
       " ['various', [0, 1, 2, 3]],\n",
       " ['follow', [0, 1, 2, 3, 4, 5, 6, 8, 9]],\n",
       " ['14', [0, 2, 3, 5, 6, 8]],\n",
       " ['duo', [0]],\n",
       " ['combination', [0, 1, 9]],\n",
       " ['baricitinib', [0]],\n",
       " ['remdesivir', [0, 2]],\n",
       " ['shave', [0]],\n",
       " ['recovery', [0, 1, 8]],\n",
       " ['hospitalize', [0, 1, 2, 3, 4]],\n",
       " ['recommends', [0]],\n",
       " ['world', [0]],\n",
       " ['organization', [0, 2, 4]],\n",
       " ['caution', [0]],\n",
       " ['therapy', [0, 1, 2, 3, 4, 6, 8]],\n",
       " ['andre', [0]],\n",
       " ['kalil', [0]],\n",
       " ['nebraska', [0]],\n",
       " ['omaha', [0]],\n",
       " ['anti-inflammatory', [0]],\n",
       " ['500', [0]],\n",
       " ['moderate', [0, 2, 4, 9]],\n",
       " ['c.', [0]],\n",
       " ['control', [0, 1, 2, 3, 5, 7, 8, 9]],\n",
       " ['monitor', [0, 1, 2, 3, 8]],\n",
       " ['long', [0, 1, 3, 4, 7, 8, 9]],\n",
       " ['take', [0, 1, 5, 6, 7, 8, 9]],\n",
       " ['without', [0, 2, 5, 6, 7, 8, 9]],\n",
       " ['sustained', [0]],\n",
       " ['care', [0, 1, 2, 3, 4, 5, 7, 8]],\n",
       " ['median', [0, 1, 2, 3, 4, 8]],\n",
       " ['compare', [0, 1, 2, 3, 4, 6, 7, 8, 9]],\n",
       " ['eight', [0, 4]],\n",
       " ['edge', [0]],\n",
       " ['invasive', [0, 1, 2, 8]],\n",
       " ['ventilation', [0, 1, 2, 8]],\n",
       " ['fell', [0]],\n",
       " ['alone', [0, 1, 2, 9]],\n",
       " ['save', [0, 1, 9]],\n",
       " ['live', [0, 1, 7, 8, 9]],\n",
       " ['scarce', [0]],\n",
       " ['front-line', [0]],\n",
       " ['health-care', [0, 7]],\n",
       " ['worker', [0, 5]],\n",
       " ['probably', [0]],\n",
       " ['get', [0, 4]],\n",
       " ['next', [0, 3, 7]],\n",
       " ['queue', [0]],\n",
       " ['supply', [0]],\n",
       " ['elderly', [0, 9]],\n",
       " ['kate', [0]],\n",
       " ['bubar', [0]],\n",
       " ['daniel', [0, 1]],\n",
       " ['larremore', [0]],\n",
       " ['colorado', [0]],\n",
       " ['boulder', [0]],\n",
       " ['roll', [0]],\n",
       " ['priority', [0, 2, 5, 7]],\n",
       " ['k.', [0]],\n",
       " ['influence', [0]],\n",
       " ['population', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['delivery', [0, 9]],\n",
       " ['offer', [0, 7]],\n",
       " ['scenario', [0, 9]],\n",
       " ['jabs', [0]],\n",
       " ['older', [0, 1, 5, 9]],\n",
       " ['60', [0, 1, 2]],\n",
       " ['life', [0, 1, 7, 8, 9]],\n",
       " ['possible', [0, 3, 4, 6, 9]],\n",
       " ['infected', [0, 1, 3, 4, 9]],\n",
       " ['prioritize', [0, 5]],\n",
       " ['young', [0, 9]],\n",
       " ['deaths', [0]],\n",
       " ['hard-hit', [0]],\n",
       " ['achieve', [0, 3]],\n",
       " ['indicates', [0]],\n",
       " ['history', [0, 3, 8]],\n",
       " ['recent', [0, 3, 4, 5, 9]],\n",
       " ['lasting', [0]],\n",
       " ['front-runner', [0]],\n",
       " ['still', [0, 7, 8, 9]],\n",
       " ['potent', [0, 4]],\n",
       " ['jab', [0]],\n",
       " ['biotech', [0]],\n",
       " ['firm', [0]],\n",
       " ['report', [0, 1, 2, 3, 4, 6, 8, 9]],\n",
       " ['gauge', [0]],\n",
       " ['whether', [0, 1, 2, 4, 8, 9]],\n",
       " ['alicia', [0]],\n",
       " ['widge', [0]],\n",
       " ['allergy', [0, 4]],\n",
       " ['infectious', [0, 4, 5, 8]],\n",
       " ['bethesda', [0]],\n",
       " ['maryland', [0, 5]],\n",
       " ['34', [0, 8]],\n",
       " ['t.', [0]],\n",
       " ['latch', [0]],\n",
       " ['key', [0, 2, 3, 4, 8, 9]],\n",
       " ['peak', [0]],\n",
       " ['1–2', [0, 3, 6]],\n",
       " ['slightly', [0, 4]],\n",
       " ['subsequent', [0, 3, 4, 8]],\n",
       " ['2.5', [0, 4]],\n",
       " ['neutralizing', [0, 1, 3, 4]],\n",
       " ['none', [0, 1, 4]],\n",
       " ['experience', [0, 3, 5, 7, 9]],\n",
       " ['vaccine-related', [0]],\n",
       " ['durable', [0]],\n",
       " ['pinch', [0]],\n",
       " ['protect', [0, 9]],\n",
       " ['sufficient', [0, 3, 7]],\n",
       " ['monkey', [0]],\n",
       " ['contribute', [0, 1, 3, 4, 6, 9]],\n",
       " ['immunity', [0, 3, 9]],\n",
       " ['easy', [0]],\n",
       " ['way', [0, 2, 4, 8, 9]],\n",
       " ['predict', [0, 3, 4, 8, 9]],\n",
       " ['aspect', [0, 8]],\n",
       " ['dan', [0]],\n",
       " ['barouch', [0]],\n",
       " ['harvard', [0]],\n",
       " ['understand', [0, 2, 3, 8, 9]],\n",
       " ['element', [0, 7]],\n",
       " ['defend', [0]],\n",
       " ['rhesus', [0]],\n",
       " ['macaque', [0]],\n",
       " ['macaca', [0]],\n",
       " ['multatta', [0]],\n",
       " ['mcmahan', [0]],\n",
       " ['recipient', [0, 1]],\n",
       " ['animal', [0, 4, 9]],\n",
       " ['boost', [0, 9]],\n",
       " ['activation', [0, 8]],\n",
       " ['antibody-dependent', [0]],\n",
       " ['natural', [0, 3, 4, 8, 9]],\n",
       " ['killer', [0]],\n",
       " ['higher', [0, 3, 4, 8, 9]],\n",
       " ['confer', [0, 9]],\n",
       " ['lower', [0, 3, 4, 8]],\n",
       " ['cd8+', [0]],\n",
       " ['re-infection', [0, 8]],\n",
       " ['suggests', [0, 3, 6]],\n",
       " ['smell', [0, 8]],\n",
       " ['sniff', [0]],\n",
       " ['count', [0, 8]],\n",
       " ['fast', [0, 9]],\n",
       " ['cheap', [0]],\n",
       " ['stop', [0, 1]],\n",
       " ['outbreak', [0, 1, 5, 7, 9]],\n",
       " ['previous', [0, 1, 3, 7, 8, 9]],\n",
       " ['three-quarters', [0]],\n",
       " ['lose', [0, 9]],\n",
       " ['sense', [0, 4]],\n",
       " ['statistic', [0]],\n",
       " ['hold', [0, 5]],\n",
       " ['true', [0]],\n",
       " ['feel', [0]],\n",
       " ['ill.', [0]],\n",
       " ['distinctive', [0]],\n",
       " ['prompt', [0, 9]],\n",
       " ['roy', [0]],\n",
       " ['parker', [0]],\n",
       " ['mass', [0, 9]],\n",
       " ['loss', [0, 7, 8, 9]],\n",
       " ['quash', [0]],\n",
       " ['epidemic', [0, 9]],\n",
       " ['d.', [0]],\n",
       " ['b.', [0]],\n",
       " ['simulation', [0, 9]],\n",
       " ['three', [0, 1, 3, 4, 6, 8, 9]],\n",
       " ['20,000', [0]],\n",
       " ['assume', [0, 2]],\n",
       " ['detectable', [0, 1, 9]],\n",
       " ['would', [0, 1, 2, 3, 4, 7, 8, 9]],\n",
       " ['event', [0, 1, 2, 3, 4, 8]],\n",
       " ['aeroplane', [0]],\n",
       " ['flight', [0]],\n",
       " ['disclosed', [0]],\n",
       " ['advises', [0]],\n",
       " ['company', [0]],\n",
       " ['darwin', [0]],\n",
       " ['bioscience', [0]],\n",
       " ['derek', [0]],\n",
       " ['toomre', [0]],\n",
       " ['founder', [0]],\n",
       " ['smell-test', [0]],\n",
       " ['u-smell-it', [0]],\n",
       " ['let', [0]],\n",
       " ['slip', [0]],\n",
       " ['allow', [0, 2, 3, 7, 8, 9]],\n",
       " ['recognition', [0]],\n",
       " ['several', [0, 1, 3, 4, 5, 8]],\n",
       " ['manufacture', [0]],\n",
       " ['designer', [0]],\n",
       " ['monoclonal', [0, 1, 2, 3, 4]],\n",
       " ['naturally', [0]],\n",
       " ['map', [0]],\n",
       " ['eli', [0, 4]],\n",
       " ['lilly', [0, 4]],\n",
       " ['indianapolis', [0, 4]],\n",
       " ['indiana', [0, 4]],\n",
       " ['cocktail', [0, 3]],\n",
       " ['regeneron', [0, 3]],\n",
       " ['tarrytown', [0]],\n",
       " ['starr', [0]],\n",
       " ['affect', [0, 1, 2, 3, 6, 9]],\n",
       " ['segment', [0]],\n",
       " ['receptor-binding', [0, 3, 4]],\n",
       " ['enter', [0, 9]],\n",
       " ['circulate', [0, 9]],\n",
       " ['widely', [0, 1, 9]],\n",
       " ['prevalent', [0, 6]],\n",
       " ['europe', [0]],\n",
       " ['another', [0, 7, 9]],\n",
       " ['netherlands', [0]],\n",
       " ['denmark', [0]],\n",
       " ['mink', [0]],\n",
       " ['farm', [0]],\n",
       " ['2', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['sensitive', [0]],\n",
       " ['yield', [0]],\n",
       " ['false', [0]],\n",
       " ['negative', [0, 1, 3, 8]],\n",
       " ['method', [0, 1, 2, 3, 4, 8]],\n",
       " ['diagnose', [0, 4, 8]],\n",
       " ['polymerase', [0]],\n",
       " ['chain', [0, 9]],\n",
       " ['detects', [0]],\n",
       " ['genetic', [0, 6]],\n",
       " ['material', [0, 9]],\n",
       " ['nose', [0]],\n",
       " ['throat', [0, 1]],\n",
       " ['swab', [0, 3, 4, 6]],\n",
       " ['survey', [0, 6, 8]],\n",
       " ['15,000', [0]],\n",
       " ['singled', [0]],\n",
       " ['caitlin', [0]],\n",
       " ['dugdale', [0]],\n",
       " ['general', [0, 6, 7]],\n",
       " ['look', [0]],\n",
       " ['think', [0]],\n",
       " ['reason', [0, 3, 4, 7, 8]],\n",
       " ['open', [0, 1, 2, 3, 4, 9]],\n",
       " ['forum', [0]],\n",
       " ['2,700', [0]],\n",
       " ['do', [0, 6, 8]],\n",
       " ['among', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " ['received', [0, 1, 2]],\n",
       " ['2.2', [0]],\n",
       " ['initial', [0, 3]],\n",
       " ['less', [0, 1, 8]],\n",
       " ['retested', [0]],\n",
       " ['especially', [0, 2, 9]],\n",
       " ['area', [0, 2, 3]],\n",
       " ['widespread', [0]],\n",
       " ['avoid', [0, 1, 7, 9]],\n",
       " ['beware', [0]],\n",
       " ['near', [0, 7]],\n",
       " ['dear', [0]],\n",
       " ['far-reaching', [0]],\n",
       " ['china', [0, 2, 7, 8]],\n",
       " ['hunan', [0]],\n",
       " ['province', [0]],\n",
       " ['encounter', [0, 7]],\n",
       " ['member', [0, 1, 3, 5, 7]],\n",
       " ['household', [0]],\n",
       " ['kaiyuan', [0]],\n",
       " ['sun', [0]],\n",
       " ['hongjie', [0]],\n",
       " ['yu', [0]],\n",
       " ['fudan', [0]],\n",
       " ['shanghai', [0]],\n",
       " ['1,178', [0]],\n",
       " ['together', [0, 3, 6, 8]],\n",
       " ['pose', [0, 6]],\n",
       " ['extend', [0, 9]],\n",
       " ['family', [0, 1, 3, 7, 8]],\n",
       " ['social', [0, 2, 5, 7, 8, 9]],\n",
       " ['transport', [0, 1]],\n",
       " ['extra', [0]],\n",
       " ['raise', [0, 8]],\n",
       " ['actually', [0]],\n",
       " ['increase', [0, 1, 2, 3, 4, 6, 7, 8]],\n",
       " ['whose', [0, 3, 8]],\n",
       " ['spend', [0, 7]],\n",
       " ['normal', [0, 1, 3, 7, 8]],\n",
       " ['home', [0, 1, 5, 7, 8]],\n",
       " ['period', [0, 1, 2, 3, 6, 8, 9]],\n",
       " ['20', [0, 2, 4, 8, 9]],\n",
       " ['persist', [0, 4, 8, 9]],\n",
       " ['beyond', [0, 3]],\n",
       " ['memory', [0, 9]],\n",
       " ['lingers', [0]],\n",
       " ['six', [0, 1, 2, 6, 9]],\n",
       " ['sporadic', [0]],\n",
       " ['account', [0, 7]],\n",
       " ['reinfection', [0, 9]],\n",
       " ['rapidly', [0, 2, 4, 9]],\n",
       " ['dwindle', [0]],\n",
       " ['shane', [0]],\n",
       " ['crotty', [0]],\n",
       " ['la', [0, 1, 4]],\n",
       " ['jolla', [0]],\n",
       " ['immunology', [0]],\n",
       " ['marker', [0, 4]],\n",
       " ['185', [0]],\n",
       " ['range', [0, 1, 2, 3, 4, 8, 9]],\n",
       " ['tend', [0]],\n",
       " ['defender', [0]],\n",
       " ['b', [0, 8]],\n",
       " ['jump-start', [0]],\n",
       " ['production', [0]],\n",
       " ['re-encountered', [0]],\n",
       " ['cd4+', [0]],\n",
       " ['peer-reviewed', [0]],\n",
       " ['19', [0, 8, 9]],\n",
       " ['mutates', [0]],\n",
       " ['race', [0, 2]],\n",
       " ['adapt', [0, 3]],\n",
       " ['breed', [0]],\n",
       " ['neovison', [0]],\n",
       " ['vison', [0]],\n",
       " ['mustela', [0]],\n",
       " ['lutreola', [0]],\n",
       " ['across', [0, 4, 7, 8, 9]],\n",
       " ['unite', [0]],\n",
       " ['since', [0, 1, 2, 3, 4, 6]],\n",
       " ['april', [0, 6]],\n",
       " ['françois', [0]],\n",
       " ['balloux', [0]],\n",
       " ['239', [0]],\n",
       " ['farmed', [0]],\n",
       " ['van', [0]],\n",
       " ['dorp', [0]],\n",
       " ['instance', [0, 4]],\n",
       " ['jump', [0]],\n",
       " ['23', [0, 1, 8, 9]],\n",
       " ['arise', [0, 9]],\n",
       " ['independently', [0, 3]],\n",
       " ['twice', [0]],\n",
       " ['frequent', [0, 4, 6, 9]],\n",
       " ['appear', [0, 1, 2, 3, 4, 7, 9]],\n",
       " ['encode', [0]],\n",
       " ['coronaviruses', [0, 9]],\n",
       " ['back', [0]],\n",
       " ['human', [0, 2, 3, 5, 9]],\n",
       " ['17', [0, 2, 3, 4, 6, 8, 9]],\n",
       " ['cold', [0, 9]],\n",
       " ['although', [0, 1, 2, 3, 4, 5, 7, 8]],\n",
       " ['deadly', [0]],\n",
       " ['mild-mannered', [0]],\n",
       " ['cousin', [0]],\n",
       " ['seasonal', [0, 9]],\n",
       " ['shield', [0, 9]],\n",
       " ['scott', [0, 4]],\n",
       " ['hensley', [0]],\n",
       " ['pennsylvania', [0]],\n",
       " ['philadelphia', [0]],\n",
       " ['anderson', [0]],\n",
       " ['pre-pandemic', [0]],\n",
       " ['oc43', [0]],\n",
       " ['one-quarter', [0]],\n",
       " ['developed', [0]],\n",
       " ['common-cold', [0]],\n",
       " ['catch', [0]],\n",
       " ['become', [0, 1, 2, 3, 4, 5, 9]],\n",
       " ['similar', [0, 2, 3, 4, 8, 9]],\n",
       " ['neither', [0, 7]],\n",
       " ['user-friendly', [0]],\n",
       " ['standard', [0, 1, 2, 4]],\n",
       " ['diagnostic', [0, 8]],\n",
       " ['assay', [0, 1, 3, 4, 8]],\n",
       " ['assessment', [0, 2, 3, 4, 6, 8]],\n",
       " ['tell', [0]],\n",
       " ['accuracy', [0, 1, 2, 3, 8]],\n",
       " ['antigen-based', [0]],\n",
       " ['antigens', [0]],\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_inverse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
